# Sign Language to Speech Conversion System

## ğŸ“Œ Overview
The **Sign Language to Speech Conversion System** is a machine learningâ€“based application designed to help bridge the communication gap between **hearing-impaired, speech-impaired, and normal individuals**. The system recognizes hand gestures used in sign language and converts them into **audible speech** in real time.

By using computer vision and deep learning techniques, this project enables natural and interactive communication through a webcam-based gesture recognition system.

---

## ğŸ¯ Project Objectives
- To recognize sign language hand gestures accurately
- To convert recognized gestures into readable text
- To transform text output into audible speech
- To enable real-time interaction using a webcam
- To improve accessibility for differently-abled individuals

---

## ğŸ§  Working Principle
The system works in the following stages:

1. **Gesture Capture**  
   Hand gestures are captured in real time using a webcam.

2. **Image Processing**  
   The captured frames are processed using OpenCV to prepare them for prediction.

3. **Gesture Recognition**  
   A deep learning model trained using **Google Teachable Machine** and implemented with **Keras** classifies the hand gestures.

4. **Text Mapping**  
   Each recognized gesture is mapped to its corresponding text representation.

5. **Speech Output**  
   The text output is converted into audible speech using a text-to-speech module.

---

## ğŸ—ï¸ System Architecture
- **Input:** Webcam (hand gesture images)
- **Processing:** OpenCV for image handling
- **Model:** Keras-based deep learning model
- **Output:** Text display and voice output

---

## âš™ï¸ Technology Stack

### Programming Language
- Python

### Libraries & Frameworks
- Keras
- TensorFlow
- OpenCV
- NumPy
- Text-to-Speech (TTS)

### Tools
- Google Teachable Machine
- Google Colab
- GitHub

---

## ğŸ“ Project Structure
â”œâ”€â”€ training_data/ # Training images for sign language gestures
â”œâ”€â”€ new_data/ # New gesture samples for testing
â”œâ”€â”€ datacollection.py # Script to collect gesture images
â”œâ”€â”€ text_test.py # Gesture recognition with text output
â”œâ”€â”€ voice_test.py # Gesture recognition with speech output
â”œâ”€â”€ *.h5 # Trained Keras model file
â”œâ”€â”€ README.md # Project documentation

---

## ğŸš€ Key Features
- Real-time sign language recognition
- Deep learning-based gesture classification
- Webcam-based live detection
- Conversion of gestures into speech
- Simple and easy-to-use system

---

## ğŸ§ª Testing
The system was tested under:
- Different lighting conditions
- Various hand orientations
- Real-time webcam input

The model showed reliable performance for gestures included in the training dataset.

---

## ğŸ“Š Results
- Accurate recognition of trained sign language gestures
- Real-time text and speech output
- Effective communication support for differently-abled users

---

## ğŸ”® Future Enhancements
- Support for complete sentences in sign language
- Multi-language speech output
- Improved accuracy using advanced CNN models
- Mobile and web application deployment
- Integration with assistive devices

---

## ğŸ‘¨â€ğŸ’» Author
**Bhargav C**  
Bachelor of Engineering â€“ Computer Science & Engineering  
Bengaluru, India  

---

## ğŸ“„ License
This project is developed for academic and educational purposes.
